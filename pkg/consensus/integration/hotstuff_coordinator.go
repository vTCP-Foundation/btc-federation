package integration

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/rs/zerolog"

	"btc-federation/pkg/consensus/crypto"
	"btc-federation/pkg/consensus/engine"
	"btc-federation/pkg/consensus/events"
	"btc-federation/pkg/consensus/messages"
	"btc-federation/pkg/consensus/network"
	"btc-federation/pkg/consensus/storage"
	"btc-federation/pkg/consensus/types"
)

// HotStuffCoordinator implements the complete HotStuff protocol following the data flow diagram exactly.
// This is the bulletproof implementation for high-stakes environments.
type HotStuffCoordinator struct {
	// Node configuration
	nodeID     types.NodeID
	validators []types.NodeID
	config     *types.ConsensusConfig
	
	// Infrastructure
	consensus   *engine.HotStuffConsensus
	network     network.NetworkInterface
	storage     storage.StorageInterface
	crypto      crypto.CryptoInterface
	eventTracer events.EventTracer
	logger      zerolog.Logger
	
	// Current state
	currentView  types.ViewNumber
	currentPhase types.ConsensusPhase
	
	// Timeout and view change state
	viewTimer         *time.Timer
	timeoutStarted    bool
	timeoutMessages   map[types.ViewNumber]map[types.NodeID]*messages.TimeoutMsg
	newViewMessages   map[types.ViewNumber]map[types.NodeID]*messages.NewViewMsg
	highestQC         *types.QuorumCertificate
	
	// Phase state tracking per block (prevent duplicate phase transitions)
	processedPhases map[types.BlockHash]map[types.ConsensusPhase]bool
	// Track processed QCs to prevent duplicate decide phase processing
	processedCommitQCs map[types.BlockHash]bool
	
	// Vote collection per phase per block
	prepareVotes    map[types.BlockHash]map[types.NodeID]*types.Vote
	preCommitVotes  map[types.BlockHash]map[types.NodeID]*types.Vote
	commitVotes     map[types.BlockHash]map[types.NodeID]*types.Vote
	
	// QC storage
	prepareQCs    map[types.BlockHash]*types.QuorumCertificate
	preCommitQCs  map[types.BlockHash]*types.QuorumCertificate
	commitQCs     map[types.BlockHash]*types.QuorumCertificate
	
	// Synchronization
	mu      sync.RWMutex
	ctx     context.Context
	cancel  context.CancelFunc
	started bool
}

// NewHotStuffCoordinator creates a new coordinator following the exact HotStuff protocol.
func NewHotStuffCoordinator(
	nodeID types.NodeID,
	validators []types.NodeID,
	config *types.ConsensusConfig,
	consensus *engine.HotStuffConsensus,
	network network.NetworkInterface,
	storage storage.StorageInterface,
	crypto crypto.CryptoInterface,
	eventTracer events.EventTracer,
	logger zerolog.Logger,
) *HotStuffCoordinator {
	ctx, cancel := context.WithCancel(context.Background())
	
	return &HotStuffCoordinator{
		nodeID:          nodeID,
		validators:      validators,
		config:          config,
		consensus:       consensus,
		network:         network,
		storage:         storage,
		crypto:          crypto,
		eventTracer:     eventTracer,
		logger:          logger.With().Uint16("node_id", uint16(nodeID)).Logger(),
		currentView:        0,
		currentPhase:       types.PhaseNone,
		viewTimer:          nil,
		timeoutStarted:     false,
		timeoutMessages:    make(map[types.ViewNumber]map[types.NodeID]*messages.TimeoutMsg),
		newViewMessages:    make(map[types.ViewNumber]map[types.NodeID]*messages.NewViewMsg),
		highestQC:          nil,
		processedPhases:    make(map[types.BlockHash]map[types.ConsensusPhase]bool),
		processedCommitQCs: make(map[types.BlockHash]bool),
		prepareVotes:    make(map[types.BlockHash]map[types.NodeID]*types.Vote),
		preCommitVotes:  make(map[types.BlockHash]map[types.NodeID]*types.Vote),
		commitVotes:     make(map[types.BlockHash]map[types.NodeID]*types.Vote),
		prepareQCs:      make(map[types.BlockHash]*types.QuorumCertificate),
		preCommitQCs:    make(map[types.BlockHash]*types.QuorumCertificate),
		commitQCs:       make(map[types.BlockHash]*types.QuorumCertificate),
		ctx:             ctx,
		cancel:          cancel,
	}
}

// Start initializes the coordinator.
func (hc *HotStuffCoordinator) Start() error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	if hc.started {
		return fmt.Errorf("coordinator already started")
	}
	
	// PERSISTENCE FIX: Recover state from persistent storage on startup
	if err := hc.recoverFromStorage(); err != nil {
		// Log warning but don't fail startup - this might be a fresh start
		hc.logger.Warn().
			Err(err).
			Str("action", "storage_recovery").
			Msg("Storage recovery failed, continuing with fresh state")
	}
	
	hc.started = true
	
	// Send initial NewView message to leader for view 0 (if not leader)
	if !hc.isLeader(hc.currentView) {
		go hc.sendNewViewMessageToLeader(hc.currentView)
	}
	
	return nil
}

// Stop gracefully shuts down the coordinator.
func (hc *HotStuffCoordinator) Stop() error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	if !hc.started {
		return nil
	}
	
	// Stop view timer
	hc.stopViewTimer()
	
	hc.cancel()
	hc.started = false
	return nil
}

// ProposeBlock implements lines 192-226: NewView Collection + Block Proposal Phase
func (hc *HotStuffCoordinator) ProposeBlock(payload []byte) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	if !hc.started {
		return fmt.Errorf("coordinator not started")
	}
	
	// Check if we are the leader for current view
	if !hc.isLeader(hc.currentView) {
		return fmt.Errorf("node %d is not leader for view %d", hc.nodeID, hc.currentView)
	}

	// FIX for partition test: Stop any existing timer before starting the proposal process.
	// This ensures the leader has the full timeout duration for NewView collection
	// and prevents a race condition where its own timer expires prematurely.
	hc.stopViewTimer()

	// Start view timer BEFORE NewView collection (protocol timing)
	hc.startViewTimer()
	
	// STEP 1: NewView Collection Phase (lines 195-226 in mermaid)
	if err := hc.collectNewViewMessages(); err != nil {
		return fmt.Errorf("failed to collect NewView messages: %w", err)
	}

	// STEP 2: Block Proposal Phase (lines 227-238 in mermaid)
	// Record proposal creation event
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventProposalCreated, events.EventPayload{
			"view":         hc.currentView,
			"payload_size": len(payload),
			"leader":       hc.nodeID,
		})
	}
	
	// Line 21-22: Get highest QC from NewView collection (not outdated PrepareQC)
	blockTree := hc.consensus.GetBlockTree()
	highestQC := hc.highestQC  // Use maxQC determined from NewView messages
	
	// Line 23-24: Get parent block from storage
	parentBlock := blockTree.GetCommitted()
	if parentBlock == nil {
		return fmt.Errorf("no committed parent block available")
	}
	
	// Line 25: Create new block with QC
	parentHash := parentBlock.Hash
	height := parentBlock.Height + 1
	block := types.NewBlock(parentHash, height, hc.currentView, hc.nodeID, payload)
	
	// Line 26-27: Sign block proposal (LeaderSig)
	blockData := fmt.Sprintf("%x:%d:%d:%d", block.Hash, block.Height, block.View, block.Proposer)
	leaderSig, err := hc.crypto.Sign([]byte(blockData))
	if err != nil {
		return fmt.Errorf("failed to sign block proposal: %w", err)
	}
	
	// Line 28-29: Broadcast ProposalMsg to all validators
	proposal := messages.NewProposalMsg(block, highestQC, hc.currentView, hc.nodeID)
	if err := hc.network.Broadcast(hc.ctx, proposal); err != nil {
		return fmt.Errorf("failed to broadcast proposal: %w", err)
	}
	
	// Record proposal broadcast event
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventProposalBroadcasted, events.EventPayload{
			"block_hash": block.Hash,
			"view":       hc.currentView,
			"leader":     hc.nodeID,
		})
	}

	// PROTOCOL FIX: Leader must also act as a validator for its own proposal
	// According to data-flow.mermaid lines 202-205: Leader broadcasts to ALL validators (including itself)
	// The leader must participate in both roles:
	// 1. Leader role: Create and broadcast proposal, collect votes
	// 2. Validator role: Validate and vote for the proposal (like any other validator)
	
	// Process proposal as leader (but also create votes like a validator) to ensure vote creation and processing
	if err := hc.processProposalInternal(proposal, true); err != nil {
		return fmt.Errorf("leader failed to validate own proposal: %w", err)
	}
	
	// Timer already started before NewView collection - no need to restart here
	
	hc.logger.Info().
		Hex("block_hash", block.Hash[:]).
		Hex("parent_hash", block.ParentHash[:]).
		Uint64("view", uint64(hc.currentView)).
		Uint64("height", uint64(block.Height)).
		Uint16("proposer", uint16(block.Proposer)).
		Int("payload_size", len(block.Payload)).
		Str("phase", "proposal").
		Msg("Block proposed successfully")
	_ = leaderSig // Store signature in proposal in production
	return nil
}

// ProcessProposal implements lines 34-54: Prepare Phase Processing
func (hc *HotStuffCoordinator) ProcessProposal(proposal *messages.ProposalMsg) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	// Record proposal received event (for non-leader nodes)
	if hc.eventTracer != nil && !hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventProposalReceived, events.EventPayload{
			"block_hash": proposal.Block.Hash,
			"view":       proposal.View(),
			"leader":     proposal.Block.Proposer,
		})
	}
	
	// Use internal processing method for followers (not leader)
	return hc.processProposalInternal(proposal, false)
}

// processProposalInternal handles proposal processing for the leader without duplicate block addition
func (hc *HotStuffCoordinator) processProposalInternal(proposal *messages.ProposalMsg, isLeader bool) error {
	// ASSERTION: Verify coordinator and engine views are synchronized
	if hc.consensus.GetCurrentView() != hc.currentView {
		return fmt.Errorf("view desynchronized: coordinator view %d != engine view %d (critical bug)",
			hc.currentView, hc.consensus.GetCurrentView())
	}
	
	// Line 38: Validate view number
	if proposal.View() != hc.currentView {
		return fmt.Errorf("proposal view %d != current view %d", proposal.View(), hc.currentView)
	}

	// Lines 39-42: Check parent exists and validate
	blockTree := hc.consensus.GetBlockTree()
	block := proposal.Block
	if !block.IsGenesis() {
		parentBlock, err := blockTree.GetBlock(block.ParentHash)
		if err != nil {
			return fmt.Errorf("parent block not found: %w", err)
		}
		
		// Line 43: Ensure parent is highestQC.block or descendant of locked block
		if parentBlock.Height+1 != block.Height {
			return fmt.Errorf("invalid block height: expected %d, got %d", parentBlock.Height+1, block.Height)
		}
	}

	// Line 44-45: Verify proposal signature
	expectedLeader := hc.getLeader(proposal.View())
	if proposal.Sender() != expectedLeader {
		return fmt.Errorf("proposal sender %d != expected leader %d for view %d", 
			proposal.Sender(), expectedLeader, proposal.View())
	}

	// Line 46: Apply SAFENODE safety predicate (mermaid line 256)
	// SAFENODE: (block extends from lockedQC.block) OR (justify.view > lockedQC.view)
	lockedQC := hc.consensus.GetLockedQC()
	justifyQC := proposal.HighQC
	
	// SECURITY FIX: Verify HighQC signatures before using it for safety decisions
	if justifyQC != nil {
		if err := hc.verifyQC(justifyQC); err != nil {
			return fmt.Errorf("invalid HighQC in proposal: %w", err)
		}
	}
	
	// CRITICAL: Verify parent == justify.block (data flow line 228)
	// This prevents byzantine leaders from creating invalid block chains
	if justifyQC != nil && !block.IsGenesis() {
		if block.ParentHash != justifyQC.BlockHash {
			// Record validation failure for byzantine behavior detection
			if hc.eventTracer != nil && !isLeader {
				hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventProposalRejected, events.EventPayload{
					"block_hash": block.Hash,
					"view":       proposal.View(),
					"reason":     "parent_mismatch",
					"parent":     block.ParentHash,
					"justify":    justifyQC.BlockHash,
				})
			}
			return fmt.Errorf("parent block %x != justify.block %x (byzantine leader behavior)", 
				block.ParentHash, justifyQC.BlockHash)
		}
	}
	
	safeNodePassed := false
	if lockedQC == nil {
		// No locked QC yet, so safe to vote
		safeNodePassed = true
	} else if justifyQC != nil && justifyQC.View > lockedQC.View {
		// Justify QC has higher view than locked QC
		safeNodePassed = true
	} else {
		// SECURITY FIX: Proper chain validation - block must extend from lockedQC.block
		blockTree := hc.consensus.GetBlockTree()
		if lockedQC.BlockHash == block.Hash {
			// Block is exactly the locked block
			safeNodePassed = true
		} else {
			// Check if block extends from locked QC block (proper ancestry check)
			isAncestor, err := blockTree.IsAncestor(lockedQC.BlockHash, block.Hash)
			if err != nil {
				return fmt.Errorf("failed to verify block ancestry for SafeNode check: %w", err)
			}
			safeNodePassed = isAncestor
		}
	}
	
	// Emit SafeNode check result only for validators, not leader
	if hc.eventTracer != nil && !isLeader {
		payload := events.EventPayload{
			"block_hash": block.Hash,
			"view":       proposal.View(),
		}
		
		if lockedQC != nil {
			payload["locked_view"] = lockedQC.View
		} else {
			payload["locked_view"] = -1
		}
		
		if justifyQC != nil {
			payload["justify_view"] = justifyQC.View
		} else {
			payload["justify_view"] = -1
		}
		
		if safeNodePassed {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventSafeNodePassed, payload)
		} else {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventSafeNodeFailed, payload)
			return fmt.Errorf("SafeNode predicate failed")
		}
	}
	
	// Continue with consensus engine processing
	vote, err := hc.consensus.ProcessProposal(block)
	if err != nil {
		return fmt.Errorf("safety rules rejected proposal: %w", err)
	}

	// Record validation events only for validators, not for leader processing its own proposal
	if hc.eventTracer != nil && !isLeader {
		hc.emitValidatorEvent(events.EventProposalValidated, events.EventPayload{
			"block_hash": block.Hash,
			"view":       proposal.View(),
			"valid":      vote != nil,
			"safenode":   safeNodePassed,
		})
		
		// Emit block added event (only for validators)
		hc.emitValidatorEvent(events.EventBlockAdded, events.EventPayload{
			"block_hash":   block.Hash,
			"parent_hash":  block.ParentHash,
			"height":       block.Height,
			"view":         block.View,
		})
	}

	// Line 47-50: Block addition is handled by consensus engine's ProcessProposal

	// Line 51-52: Sign prepare vote
	if vote != nil {
		// Record prepare vote creation event for all validators (including leader as validator)
		if hc.eventTracer != nil {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPrepareVoteCreated, events.EventPayload{
				"block_hash": vote.BlockHash,
				"view":       vote.View,
				"phase":      vote.Phase,
				"voter":      vote.Voter,
			})
		}
		
		// Sign the vote with our crypto interface
		voteData := fmt.Sprintf("%x:%d:%s:%d", vote.BlockHash, vote.View, vote.Phase, vote.Voter)
		signature, err := hc.crypto.Sign([]byte(voteData))
		if err != nil {
			return fmt.Errorf("failed to sign prepare vote: %w", err)
		}
		vote.Signature = signature
		// Line 53-54: Send vote to leader (validators send over network, leader processes locally)  
		currentLeader := hc.getLeader(hc.currentView)
		if hc.nodeID != currentLeader {
			// Validator: Send vote over network to leader
			voteMsg := messages.NewVoteMsg(vote, hc.nodeID)
			if err := hc.network.Send(hc.ctx, currentLeader, voteMsg); err != nil {
				return fmt.Errorf("failed to send vote: %w", err)
			}
			
			// Record prepare vote sent event only for validators (not leader)
			if hc.eventTracer != nil {
				hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPrepareVoteSent, events.EventPayload{
					"block_hash": vote.BlockHash,
					"view":       vote.View,
					"phase":      vote.Phase,
					"leader":     currentLeader,
				})
			}
		} else {
			// Leader: Don't send to self over network, will process vote directly below
			// Record prepare vote sent event for leader (conceptual - leader "sends" to itself)
			if hc.eventTracer != nil {
				hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPrepareVoteSent, events.EventPayload{
					"block_hash": vote.BlockHash,
					"view":       vote.View,
					"phase":      vote.Phase,
					"leader":     currentLeader,
				})
			}
		}
		
		// Process our own vote without emitting "received" or "validated" events (validators don't validate their own votes)
		if err := hc.processVoteInternalWithAllEvents(vote, false, false); err != nil {
			return fmt.Errorf("failed to process own vote: %w", err)
		}
		
		hc.logger.Info().
			Hex("block_hash", block.Hash[:]).
			Uint64("view", uint64(vote.View)).
			Str("phase", vote.Phase.String()).
			Uint16("voter", uint16(vote.Voter)).
			Msg("Vote created and sent")
	}

	// Start timer for followers when processing proposal
	if !isLeader {
		hc.startViewTimer()
	}

	hc.currentPhase = types.PhasePrepare
	return nil
}

// ProcessVote handles vote collection and QC formation following lines 77-106
func (hc *HotStuffCoordinator) ProcessVote(vote *types.Vote) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	// Vote received event is recorded in processVoteInternal to avoid duplication
	return hc.processVoteInternal(vote)
}

// processVoteInternal handles vote processing without mutex (assumes mutex is already held)
func (hc *HotStuffCoordinator) processVoteInternal(vote *types.Vote) error {
	return hc.processVoteInternalWithEvents(vote, true)
}

// processVoteInternalWithEvents handles vote processing with optional event emission
func (hc *HotStuffCoordinator) processVoteInternalWithEvents(vote *types.Vote, emitReceivedEvent bool) error {
	return hc.processVoteInternalWithAllEvents(vote, emitReceivedEvent, emitReceivedEvent)
}

// processVoteInternalWithAllEvents handles vote processing with full control over event emission
func (hc *HotStuffCoordinator) processVoteInternalWithAllEvents(vote *types.Vote, emitReceivedEvent, emitValidatedEvent bool) error {
	// ASSERTION: Verify coordinator and engine views are synchronized before vote processing
	if hc.consensus.GetCurrentView() != hc.currentView {
		return fmt.Errorf("view desynchronized before vote processing: coordinator view %d != engine view %d",
			hc.currentView, hc.consensus.GetCurrentView())
	}
	
	// Line 82-83: Verify vote signature
	voteData := fmt.Sprintf("%x:%d:%s:%d", vote.BlockHash, vote.View, vote.Phase, vote.Voter)
	if err := hc.crypto.Verify([]byte(voteData), vote.Signature, vote.Voter); err != nil {
		return fmt.Errorf("invalid vote signature: %w", err)
	}
	
	// Record phase-specific vote received event
	var voteReceivedEvent events.EventType
	switch vote.Phase {
	case types.PhasePrepare:
		voteReceivedEvent = events.EventPrepareVoteReceived
	case types.PhasePreCommit:
		voteReceivedEvent = events.EventPreCommitVoteReceived
	case types.PhaseCommit:
		voteReceivedEvent = events.EventCommitVoteReceived
	default:
		voteReceivedEvent = events.EventVoteReceived // Fallback
	}
	
	// Vote reception events should only be emitted by leaders (who collect votes)
	if emitReceivedEvent {
		hc.emitLeaderEvent(voteReceivedEvent, events.EventPayload{
			"block_hash": vote.BlockHash,
			"view":       vote.View,
			"phase":      vote.Phase,
			"voter":      vote.Voter,
		})
	}
	
	// Record phase-specific vote validation event
	var voteValidatedEvent events.EventType
	switch vote.Phase {
	case types.PhasePrepare:
		voteValidatedEvent = events.EventPrepareVoteValidated
	case types.PhasePreCommit:
		voteValidatedEvent = events.EventPreCommitVoteValidated
	case types.PhaseCommit:
		voteValidatedEvent = events.EventCommitVoteValidated
	default:
		voteValidatedEvent = events.EventVoteReceived // Fallback
	}
	
	// Vote validation events should only be emitted by leaders (who validate votes)
	if emitValidatedEvent {
		hc.emitLeaderEvent(voteValidatedEvent, events.EventPayload{
			"block_hash": vote.BlockHash,
			"view":       vote.View,
			"phase":      vote.Phase,
			"voter":      vote.Voter,
		})
	}
	
	// Store vote based on phase
	var voteMap map[types.BlockHash]map[types.NodeID]*types.Vote
	switch vote.Phase {
	case types.PhasePrepare:
		voteMap = hc.prepareVotes
	case types.PhasePreCommit:
		voteMap = hc.preCommitVotes
	case types.PhaseCommit:
		voteMap = hc.commitVotes
	default:
		return fmt.Errorf("unknown vote phase: %s", vote.Phase)
	}
	
	// Initialize vote collection for this block if needed
	if voteMap[vote.BlockHash] == nil {
		voteMap[vote.BlockHash] = make(map[types.NodeID]*types.Vote)
	}
	
	// Store vote
	voteMap[vote.BlockHash][vote.Voter] = vote
	votes := voteMap[vote.BlockHash]
	
	// Line 80-81: Check vote threshold (≥2f+1)
	requiredVotes := (len(hc.validators)*2)/3 + 1 // 2f+1
	if len(votes) >= requiredVotes {
		// Check if QC already formed for this block+phase (idempotent)
		var existingQC *types.QuorumCertificate
		switch vote.Phase {
		case types.PhasePrepare:
			existingQC = hc.prepareQCs[vote.BlockHash]
		case types.PhasePreCommit:
			existingQC = hc.preCommitQCs[vote.BlockHash]
		case types.PhaseCommit:
			existingQC = hc.commitQCs[vote.BlockHash]
		}
		
		if existingQC != nil {
			// QC already formed for this block+phase, skip
			return nil
		}
		
		// Line 84: Form Quorum Certificate (once per block+phase)
		qc, err := hc.formQuorumCertificate(vote.BlockHash, vote.View, vote.Phase, votes)
		if err != nil {
			return fmt.Errorf("failed to form QC: %w", err)
		}
		
		// Line 85-87: Store QC (fsync to disk)
		if err := hc.storeQC(qc); err != nil {
			return fmt.Errorf("failed to store QC: %w", err)
		}
		
		// Process QC based on phase (once per block+phase)
		if err := hc.processQuorumCertificate(qc); err != nil {
			return fmt.Errorf("failed to process QC: %w", err)
		}
		
		hc.logger.Info().
			Hex("block_hash", vote.BlockHash[:]).
			Str("phase", vote.Phase.String()).
			Uint64("view", uint64(vote.View)).
			Int("vote_count", len(votes)).
			Int("required_votes", int(hc.config.QuorumThreshold())).
			Str("action", "qc_formed").
			Msg("Quorum certificate formed successfully")
	}
	
	return nil
}

// processQuorumCertificate handles QC processing and phase transitions
func (hc *HotStuffCoordinator) processQuorumCertificate(qc *types.QuorumCertificate) error {
	switch qc.Phase {
	case types.PhasePrepare:
		return hc.processPreCommitPhase(qc)
	case types.PhasePreCommit:
		return hc.processCommitPhase(qc)
	case types.PhaseCommit:
		return hc.processCommitQCInternal(qc)
	default:
		return fmt.Errorf("unknown QC phase: %s", qc.Phase)
	}
}

// markPhaseProcessed marks a phase as processed for a block (idempotent state management)
func (hc *HotStuffCoordinator) markPhaseProcessed(blockHash types.BlockHash, phase types.ConsensusPhase) bool {
	if hc.processedPhases[blockHash] == nil {
		hc.processedPhases[blockHash] = make(map[types.ConsensusPhase]bool)
	}
	
	if hc.processedPhases[blockHash][phase] {
		return false // Already processed
	}
	
	hc.processedPhases[blockHash][phase] = true
	return true // First time processing
}

// processPreCommitPhase implements lines 110-131: Pre-Commit Phase
func (hc *HotStuffCoordinator) processPreCommitPhase(prepareQC *types.QuorumCertificate) error {
	// Idempotent: only process PreCommit phase once per block
	if !hc.markPhaseProcessed(prepareQC.BlockHash, types.PhasePreCommit) {
		return nil // Already processed this phase for this block
	}
	// Line 111-112: Broadcast PrepareQC to validators
	if hc.isLeader(hc.currentView) {
		if err := hc.broadcastQC(prepareQC); err != nil {
			return fmt.Errorf("failed to broadcast PrepareQC: %w", err)
		}
	}
	
	// Line 114-115: Verify all QC signatures
	if err := hc.verifyQC(prepareQC); err != nil {
		return fmt.Errorf("invalid PrepareQC: %w", err)
	}
	
	// Record PrepareQC validation event (validators only, not leader)
	if hc.eventTracer != nil && !hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPrepareQCValidated, events.EventPayload{
			"block_hash":  prepareQC.BlockHash,
			"view":        prepareQC.View,
			"phase":       prepareQC.Phase,
			"vote_count":  len(prepareQC.Votes),
		})
	}
	
	// Line 116: Process PrepareQC for prepare state only - DO NOT update lockedQC yet
	// SAFETY FIX: lockedQC update moved to Commit phase per HotStuff Algorithm 2 line 25
	if err := hc.consensus.ProcessQC(prepareQC); err != nil {
		return fmt.Errorf("failed to process PrepareQC: %w", err)
	}
	
	// Line 117-118: Sign pre-commit vote
	preCommitVote := &types.Vote{
		BlockHash: prepareQC.BlockHash,
		View:      prepareQC.View,
		Phase:     types.PhasePreCommit,
		Voter:     hc.nodeID,
	}
	
	voteData := fmt.Sprintf("%x:%d:%s:%d", preCommitVote.BlockHash, preCommitVote.View, preCommitVote.Phase, preCommitVote.Voter)
	signature, err := hc.crypto.Sign([]byte(voteData))
	if err != nil {
		return fmt.Errorf("failed to sign pre-commit vote: %w", err)
	}
	preCommitVote.Signature = signature
	
	// Record PreCommit vote creation event (validators only, not leader)
	if hc.eventTracer != nil && !hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPreCommitVoteCreated, events.EventPayload{
			"block_hash": preCommitVote.BlockHash,
			"view":       preCommitVote.View,
			"phase":      preCommitVote.Phase,
			"voter":      preCommitVote.Voter,
		})
	}
	
	// Line 119-120: Send pre-commit vote
	leader := hc.getLeader(hc.currentView)
	
	// If we are the leader, don't send vote to ourselves - process it directly
	if hc.isLeader(hc.currentView) {
		// Leaders don't emit "received" or "validated" events for their own votes
		if err := hc.processVoteInternalWithAllEvents(preCommitVote, false, false); err != nil {
			return fmt.Errorf("failed to process own pre-commit vote: %w", err)
		}
	} else {
		// Send vote to leader
		voteMsg := messages.NewVoteMsg(preCommitVote, hc.nodeID)
		if err := hc.network.Send(hc.ctx, leader, voteMsg); err != nil {
			return fmt.Errorf("failed to send pre-commit vote: %w", err)
		}
		
		// Record PreCommit vote sent event
		if hc.eventTracer != nil {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPreCommitVoteSent, events.EventPayload{
				"block_hash": preCommitVote.BlockHash,
				"view":       preCommitVote.View,
				"phase":      preCommitVote.Phase,
				"leader":     leader,
			})
		}
		
		// Also process our own vote without emitting "received" or "validated" events
		if err := hc.processVoteInternalWithAllEvents(preCommitVote, false, false); err != nil {
			return fmt.Errorf("failed to process own pre-commit vote: %w", err)
		}
	}
	
	hc.currentPhase = types.PhasePreCommit
	fmt.Printf("   [Node %d] ✅ Entered PreCommit phase for block %x\n", hc.nodeID, prepareQC.BlockHash[:8])
	return nil
}

// processCommitPhase implements lines 135-154: Commit Phase
func (hc *HotStuffCoordinator) processCommitPhase(preCommitQC *types.QuorumCertificate) error {
	// Idempotent: only process Commit phase once per block
	if !hc.markPhaseProcessed(preCommitQC.BlockHash, types.PhaseCommit) {
		return nil // Already processed this phase for this block
	}
	// Line 135-136: Broadcast PreCommitQC to validators
	if hc.isLeader(hc.currentView) {
		if err := hc.broadcastQC(preCommitQC); err != nil {
			return fmt.Errorf("failed to broadcast PreCommitQC: %w", err)
		}
	}
	
	// Line 138-139: Verify all QC signatures
	if err := hc.verifyQC(preCommitQC); err != nil {
		return fmt.Errorf("invalid PreCommitQC: %w", err)
	}
	
	// Record PreCommitQC validation event (validators only, not leader)
	if hc.eventTracer != nil && !hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPreCommitQCValidated, events.EventPayload{
			"block_hash":  preCommitQC.BlockHash,
			"view":        preCommitQC.View,
			"phase":       preCommitQC.Phase,
			"vote_count":  len(preCommitQC.Votes),
		})
	}
	
	// SAFETY FIX: Update lockedQC here (HotStuff Algorithm 2 line 25)
	// lockedQC ← m.justify (where m.justify is the PreCommitQC)
	if err := hc.consensus.UpdateLockedQC(preCommitQC); err != nil {
		return fmt.Errorf("failed to update lockedQC: %w", err)
	}
	
	// Record locked QC update event (validators only, not leader)
	if hc.eventTracer != nil && !hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventLockedQCUpdated, events.EventPayload{
			"block_hash": preCommitQC.BlockHash,
			"view":       preCommitQC.View,
			"phase":      preCommitQC.Phase,
		})
	}
	
	// Line 140-141: Sign commit vote
	commitVote := &types.Vote{
		BlockHash: preCommitQC.BlockHash,
		View:      preCommitQC.View,
		Phase:     types.PhaseCommit,
		Voter:     hc.nodeID,
	}
	
	voteData := fmt.Sprintf("%x:%d:%s:%d", commitVote.BlockHash, commitVote.View, commitVote.Phase, commitVote.Voter)
	signature, err := hc.crypto.Sign([]byte(voteData))
	if err != nil {
		return fmt.Errorf("failed to sign commit vote: %w", err)
	}
	commitVote.Signature = signature
	
	// Record Commit vote creation event (validators only, not leader)
	if hc.eventTracer != nil && !hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventCommitVoteCreated, events.EventPayload{
			"block_hash": commitVote.BlockHash,
			"view":       commitVote.View,
			"phase":      commitVote.Phase,
			"voter":      commitVote.Voter,
		})
	}
	
	// Line 142-143: Send commit vote
	leader := hc.getLeader(hc.currentView)
	
	// If we are the leader, don't send vote to ourselves - process it directly
	if hc.isLeader(hc.currentView) {
		// Leaders don't emit "received" or "validated" events for their own votes
		if err := hc.processVoteInternalWithAllEvents(commitVote, false, false); err != nil {
			return fmt.Errorf("failed to process own commit vote: %w", err)
		}
	} else {
		// Send vote to leader
		voteMsg := messages.NewVoteMsg(commitVote, hc.nodeID)
		if err := hc.network.Send(hc.ctx, leader, voteMsg); err != nil {
			return fmt.Errorf("failed to send commit vote: %w", err)
		}
		
		// Record Commit vote sent event
		if hc.eventTracer != nil {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventCommitVoteSent, events.EventPayload{
				"block_hash": commitVote.BlockHash,
				"view":       commitVote.View,
				"phase":      commitVote.Phase,
				"leader":     leader,
			})
		}
		
		// Also process our own vote without emitting "received" or "validated" events
		if err := hc.processVoteInternalWithAllEvents(commitVote, false, false); err != nil {
			return fmt.Errorf("failed to process own commit vote: %w", err)
		}
	}
	
	hc.currentPhase = types.PhaseCommit
	fmt.Printf("   [Node %d] ✅ Entered Commit phase for block %x\n", hc.nodeID, preCommitQC.BlockHash[:8])
	return nil
}

// processDecidePhase implements lines 158-170: Decide Phase & Block Commit
func (hc *HotStuffCoordinator) processDecidePhase(commitQC *types.QuorumCertificate) error {
	// Idempotent: only process CommitQC once per block
	if hc.processedCommitQCs[commitQC.BlockHash] {
		return nil // Already processed this CommitQC for this block
	}
	hc.processedCommitQCs[commitQC.BlockHash] = true
	
	// CRITICAL FIX: Ensure QC formation event is emitted BEFORE block commitment
	// This satisfies the validation rule "QC formation must precede block commitment"
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventQCFormed, events.EventPayload{
			"block_hash":  commitQC.BlockHash,
			"view":        commitQC.View,
			"phase":       commitQC.Phase,
			"vote_count":  len(commitQC.Votes),
			"required":    (len(hc.validators)*2)/3 + 1,
		})
	}
	
	// Line 158-159: Broadcast CommitQC to validators
	if hc.isLeader(hc.currentView) {
		if err := hc.broadcastQC(commitQC); err != nil {
			return fmt.Errorf("failed to broadcast CommitQC: %w", err)
		}
	}
	
	// Line 161-162: Verify all QC signatures
	if err := hc.verifyQC(commitQC); err != nil {
		return fmt.Errorf("invalid CommitQC: %w", err)
	}
	
	// Record CommitQC validation event
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventCommitQCValidated, events.EventPayload{
			"block_hash":  commitQC.BlockHash,
			"view":        commitQC.View,
			"phase":       commitQC.Phase,
			"vote_count":  len(commitQC.Votes),
		})
	}
	
	// Line 163-166: Mark block as committed
	blockTree := hc.consensus.GetBlockTree()
	block, err := blockTree.GetBlock(commitQC.BlockHash)
	if err != nil {
		return fmt.Errorf("failed to get block for commit: %w", err)
	}
	
	// Check if block is already committed to prevent double-commitment
	committedBlock := blockTree.GetCommitted()
	if committedBlock != nil && committedBlock.Hash == block.Hash {
		// Block already committed, skip
		fmt.Printf("   [Node %d] ℹ️  Block %x already committed, skipping\n", hc.nodeID, block.Hash[:8])
	} else {
		if err := blockTree.CommitBlock(block); err != nil {
			return fmt.Errorf("failed to commit block: %w", err)
		}
		
		// Record block committed event (AFTER QC formation)
		if hc.eventTracer != nil {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventBlockCommitted, events.EventPayload{
				"block_hash": block.Hash,
				"height":     block.Height,
				"view":       commitQC.View,
			})
		}
		
		// Line 167: Emit committed block event
		hc.logger.Info().
			Hex("block_hash", block.Hash[:]).
			Hex("parent_hash", block.ParentHash[:]).
			Uint64("height", uint64(block.Height)).
			Uint64("view", uint64(block.View)).
			Uint16("proposer", uint16(block.Proposer)).
			Int("payload_size", len(block.Payload)).
			Str("phase", "decide").
			Str("action", "block_committed").
			Msg("Block committed to chain")
	}
	
	// Note: View advancement is handled by the demo coordinator to ensure synchronization
	
	return nil
}

// Helper methods

func (hc *HotStuffCoordinator) formQuorumCertificate(
	blockHash types.BlockHash,
	view types.ViewNumber,
	phase types.ConsensusPhase,
	votes map[types.NodeID]*types.Vote,
) (*types.QuorumCertificate, error) {
	requiredVotes := (len(hc.validators)*2)/3 + 1
	if len(votes) < requiredVotes {
		return nil, fmt.Errorf("insufficient votes: got %d, need %d", len(votes), requiredVotes)
	}
	
	// Convert map to slice
	voteSlice := make([]types.Vote, 0, len(votes))
	for _, vote := range votes {
		voteSlice = append(voteSlice, *vote)
		if len(voteSlice) >= requiredVotes {
			break // Take exactly 2f+1 votes
		}
	}
	
	qc := &types.QuorumCertificate{
		BlockHash: blockHash,
		View:      view,
		Phase:     phase,
		Votes:     voteSlice,
	}
	
	// Record phase-specific QC formation event
	var qcFormedEvent events.EventType
	switch phase {
	case types.PhasePrepare:
		qcFormedEvent = events.EventPrepareQCFormed
	case types.PhasePreCommit:
		qcFormedEvent = events.EventPreCommitQCFormed
	case types.PhaseCommit:
		qcFormedEvent = events.EventCommitQCFormed
	default:
		qcFormedEvent = events.EventQCFormed // Fallback
	}
	
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), qcFormedEvent, events.EventPayload{
			"block_hash":  blockHash,
			"view":        view,
			"phase":       phase,
			"vote_count":  len(voteSlice),
			"required":    requiredVotes,
		})
	}
	
	return qc, nil
}

func (hc *HotStuffCoordinator) storeQC(qc *types.QuorumCertificate) error {
	// Store in appropriate map
	switch qc.Phase {
	case types.PhasePrepare:
		hc.prepareQCs[qc.BlockHash] = qc
	case types.PhasePreCommit:
		hc.preCommitQCs[qc.BlockHash] = qc
	case types.PhaseCommit:
		hc.commitQCs[qc.BlockHash] = qc
	}
	
	// Update highest QC for timeout/view change protocol
	hc.updateHighestQC(qc)
	
	// Record phase-specific storage transaction begin event - ONLY for leader
	var beginEventType events.EventType
	var commitEventType events.EventType
	switch qc.Phase {
	case types.PhasePrepare:
		beginEventType = events.EventPrepareStorageTxBegin
		commitEventType = events.EventPrepareStorageTxCommit
	case types.PhasePreCommit:
		beginEventType = events.EventPreCommitStorageTxBegin
		commitEventType = events.EventPreCommitStorageTxCommit
	case types.PhaseCommit:
		beginEventType = events.EventCommitStorageTxBegin
		commitEventType = events.EventCommitStorageTxCommit
	default:
		beginEventType = events.EventStorageTxBegin // Fallback
		commitEventType = events.EventStorageTxCommit // Fallback
	}
	
	hc.emitLeaderEvent(beginEventType, events.EventPayload{
		"operation": "store_qc",
		"phase":     qc.Phase,
		"block_hash": qc.BlockHash,
		"view":      qc.View,
	})
	
	// Line 85-87, 128-130, 151-153: Store QC (fsync to disk)
	// PERSISTENCE FIX: Store the complete QC data, not just the view
	if err := hc.storage.StoreQC(qc); err != nil {
		return fmt.Errorf("failed to persist QC to storage: %w", err)
	}
	
	// Also store view for compatibility with existing view tracking
	if err := hc.storage.StoreView(qc.View); err != nil {
		return fmt.Errorf("failed to persist QC view to storage: %w", err)
	}
	
	// Record phase-specific storage transaction commit event - ONLY for leader
	hc.emitLeaderEvent(commitEventType, events.EventPayload{
		"operation": "store_qc",
		"phase":     qc.Phase,
		"block_hash": qc.BlockHash,
		"view":      qc.View,
	})
	
	fmt.Printf("   [Node %d] 💾 Stored %s QC for block %x (fsync to disk)\n", 
		hc.nodeID, qc.Phase, qc.BlockHash[:8])
	
	return nil
}

func (hc *HotStuffCoordinator) verifyQC(qc *types.QuorumCertificate) error {
	requiredVotes := (len(hc.validators)*2)/3 + 1
	if len(qc.Votes) < requiredVotes {
		return fmt.Errorf("insufficient votes in QC: got %d, need %d", len(qc.Votes), requiredVotes)
	}
	
	// Verify each vote signature
	for _, vote := range qc.Votes {
		voteData := fmt.Sprintf("%x:%d:%s:%d", vote.BlockHash, vote.View, vote.Phase, vote.Voter)
		if err := hc.crypto.Verify([]byte(voteData), vote.Signature, vote.Voter); err != nil {
			return fmt.Errorf("invalid vote signature from node %d: %w", vote.Voter, err)
		}
	}
	
	return nil
}

func (hc *HotStuffCoordinator) broadcastQC(qc *types.QuorumCertificate) error {
	// Strictly follow protocol lines 88-89, 111-112, 135-136, 158-159
	var qcMsg *messages.QCMsg
	
	switch qc.Phase {
	case types.PhasePrepare:
		// Line 88-89: Broadcast(PrepareQC)
		qcMsg = messages.NewPrepareQCMsg(qc, hc.nodeID)
	case types.PhasePreCommit:
		// Line 135-136: Broadcast(PreCommitQC)  
		qcMsg = messages.NewPreCommitQCMsg(qc, hc.nodeID)
	case types.PhaseCommit:
		// Line 158-159: Broadcast(CommitQC)
		qcMsg = messages.NewCommitQCMsg(qc, hc.nodeID)
	default:
		return fmt.Errorf("cannot broadcast QC for unknown phase: %s", qc.Phase)
	}
	
	// Broadcast to all validators as specified in protocol
	if err := hc.network.Broadcast(hc.ctx, qcMsg); err != nil {
		return fmt.Errorf("failed to broadcast %s QC: %w", qc.Phase, err)
	}
	
	// Record phase-specific QC broadcast event
	var qcBroadcastEvent events.EventType
	switch qc.Phase {
	case types.PhasePrepare:
		qcBroadcastEvent = events.EventPrepareQCBroadcasted
	case types.PhasePreCommit:
		qcBroadcastEvent = events.EventPreCommitQCBroadcasted
	case types.PhaseCommit:
		qcBroadcastEvent = events.EventCommitQCBroadcasted
	}
	
	if hc.eventTracer != nil && qcBroadcastEvent != "" {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), qcBroadcastEvent, events.EventPayload{
			"block_hash": qc.BlockHash,
			"view":       qc.View,
			"phase":      qc.Phase,
			"vote_count": len(qc.Votes),
		})
	}
	
	fmt.Printf("   [Node %d] 📡 Broadcast %s QC for block %x\n", 
		hc.nodeID, qc.Phase, qc.BlockHash[:8])
	
	return nil
}

func (hc *HotStuffCoordinator) isLeader(view types.ViewNumber) bool {
	return hc.getLeader(view) == hc.nodeID
}

func (hc *HotStuffCoordinator) getLeader(view types.ViewNumber) types.NodeID {
	return hc.validators[int(view)%len(hc.validators)]
}

// emitLeaderEvent emits an event only if this node is the leader for the current view
func (hc *HotStuffCoordinator) emitLeaderEvent(eventType events.EventType, payload events.EventPayload) {
	if hc.eventTracer != nil && hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), eventType, payload)
	}
}

// emitValidatorEvent emits an event only if this node is a validator (not leader) for the current view
func (hc *HotStuffCoordinator) emitValidatorEvent(eventType events.EventType, payload events.EventPayload) {
	if hc.eventTracer != nil && !hc.isLeader(hc.currentView) {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), eventType, payload)
	}
}

// emitAllNodesEvent emits an event regardless of role (for events that all nodes should emit)
func (hc *HotStuffCoordinator) emitAllNodesEvent(eventType events.EventType, payload events.EventPayload) {
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), eventType, payload)
	}
}

func (hc *HotStuffCoordinator) advanceView() {
	hc.currentView++
	hc.currentPhase = types.PhaseNone
	
	// CRITICAL: Synchronize consensus engine view with coordinator view
	// This fixes the "block view X does not match current view Y" errors
	for hc.consensus.GetCurrentView() < hc.currentView {
		hc.consensus.AdvanceView()
	}
	
	// ASSERTION: Verify synchronization succeeded
	if hc.consensus.GetCurrentView() != hc.currentView {
		panic(fmt.Sprintf("FATAL: Failed to synchronize views after advance - coordinator: %d, engine: %d",
			hc.currentView, hc.consensus.GetCurrentView()))
	}
	
	fmt.Printf("   [Node %d] 📈 Advanced to view %d (consensus engine synchronized)\n", 
		hc.nodeID, hc.currentView)
}

// Getters for testing and monitoring
func (hc *HotStuffCoordinator) GetCurrentView() types.ViewNumber {
	hc.mu.RLock()
	defer hc.mu.RUnlock()
	return hc.currentView
}

func (hc *HotStuffCoordinator) GetCurrentPhase() types.ConsensusPhase {
	hc.mu.RLock()
	defer hc.mu.RUnlock()
	return hc.currentPhase
}

// ProcessPrepareQC processes a received PrepareQC and triggers PreCommit phase
func (hc *HotStuffCoordinator) ProcessPrepareQC(qc *types.QuorumCertificate) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	// Record PrepareQC received event
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPrepareQCReceived, events.EventPayload{
			"block_hash":  qc.BlockHash,
			"view":        qc.View,
			"phase":       qc.Phase,
			"vote_count":  len(qc.Votes),
		})
	}
	
	return hc.processPreCommitPhase(qc)
}

// ProcessPreCommitQC processes a received PreCommitQC and triggers Commit phase
func (hc *HotStuffCoordinator) ProcessPreCommitQC(qc *types.QuorumCertificate) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	// Record PreCommitQC received event
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventPreCommitQCReceived, events.EventPayload{
			"block_hash":  qc.BlockHash,
			"view":        qc.View,
			"phase":       qc.Phase,
			"vote_count":  len(qc.Votes),
		})
	}
	
	return hc.processCommitPhase(qc)
}

// ProcessCommitQC processes a received CommitQC and triggers Decide phase
func (hc *HotStuffCoordinator) ProcessCommitQC(qc *types.QuorumCertificate) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	return hc.processCommitQCInternal(qc)
}

// processCommitQCInternal processes CommitQC without taking mutex (assumes mutex held)
func (hc *HotStuffCoordinator) processCommitQCInternal(qc *types.QuorumCertificate) error {
	// Record CommitQC received event
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventCommitQCReceived, events.EventPayload{
			"block_hash":  qc.BlockHash,
			"view":        qc.View,
			"phase":       qc.Phase,
			"vote_count":  len(qc.Votes),
		})
	}
	
	return hc.processDecidePhase(qc)
}

// AdvanceViewForDemo manually advances the view for demo purposes
func (hc *HotStuffCoordinator) AdvanceViewForDemo() {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	hc.advanceView()
}

// ======= TIMEOUT AND VIEW CHANGE IMPLEMENTATION =======
// Following data flow diagram lines 176-200

// startViewTimer starts the timeout timer for the current view (protocol line 177-178)
func (hc *HotStuffCoordinator) startViewTimer() {
	if !hc.config.IsTimeoutEnabled() {
		return // Timeouts disabled in config
	}
	
	// Cancel any existing timer
	hc.stopViewTimer()
	
	// Calculate timeout for current view: baseTimeout * 2^view (protocol line 178)
	timeout := hc.config.GetTimeoutForView(hc.currentView)
	
	// Start new timer
	hc.viewTimer = time.AfterFunc(timeout, func() {
		hc.onViewTimeout()
	})
	hc.timeoutStarted = true
	
    // Record view timer started event - ALL nodes emit this event since all validators participate in timeout detection
    payload := events.EventPayload{
        "view":       hc.currentView,
        "timeout":    timeout.String(),
        "timeout_ms": timeout.Milliseconds(),
    }
    // Indicate if capped at max timeout
    if timeout >= hc.config.MaxTimeout {
        payload["capped_at_max"] = true
    }
    hc.emitAllNodesEvent(events.EventViewTimerStarted, payload)
	
	fmt.Printf("   [Node %d] ⏰ Started view timer for view %d (timeout: %v)\n", 
		hc.nodeID, hc.currentView, timeout)
}

// stopViewTimer stops the current view timer
func (hc *HotStuffCoordinator) stopViewTimer() {
	if hc.viewTimer != nil {
		hc.viewTimer.Stop()
		hc.viewTimer = nil
	}
	hc.timeoutStarted = false
}

// onViewTimeout handles view timeout (protocol lines 177-188)
func (hc *HotStuffCoordinator) onViewTimeout() {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	if !hc.timeoutStarted {
		return // Timer was cancelled
	}
	
    // Emit timeout detection events - ALL nodes should emit these
    hc.emitAllNodesEvent(events.EventViewTimeout, events.EventPayload{
        "view": hc.currentView,
    })
    hc.emitAllNodesEvent(events.EventViewTimeoutDetected, events.EventPayload{
        "view": hc.currentView,
    })
	
	fmt.Printf("   [Node %d] ⏰ View %d timeout detected, initiating view change\n", 
		hc.nodeID, hc.currentView)
	
	// Protocol line 179-182: Sign and broadcast timeout message
	if err := hc.broadcastTimeoutMessage(); err != nil {
		fmt.Printf("   [Node %d] ❌ Failed to broadcast timeout: %v\n", hc.nodeID, err)
	}
	
	hc.timeoutStarted = false
}

// broadcastTimeoutMessage creates and broadcasts timeout message (protocol lines 179-182)
func (hc *HotStuffCoordinator) broadcastTimeoutMessage() error {
	// Create timeout message with highest known QC
	timeoutData := fmt.Sprintf("timeout:%d:%d", hc.currentView, hc.nodeID)
	signature, err := hc.crypto.Sign([]byte(timeoutData))
	if err != nil {
		return fmt.Errorf("failed to sign timeout message: %w", err)
	}
	
	timeoutMsg := messages.NewTimeoutMsg(hc.currentView, hc.highestQC, hc.nodeID, signature)
	
    // Broadcast timeout to all validators
    if err := hc.network.Broadcast(hc.ctx, timeoutMsg); err != nil {
        return fmt.Errorf("failed to broadcast timeout: %w", err)
    }

    // Record timeout message sent (each node sends its own timeout)
    if hc.eventTracer != nil {
        payload := events.EventPayload{
            "view":      hc.currentView,
            "sender":    hc.nodeID,
        }
        if hc.highestQC != nil {
            payload["highest_qc_view"] = hc.highestQC.View
            payload["highest_qc_phase"] = hc.highestQC.Phase
        }
        hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventTimeoutMessageSent, payload)
    }
	
	// Process our own timeout message
	if err := hc.processTimeoutMessage(timeoutMsg); err != nil {
		return fmt.Errorf("failed to process own timeout: %w", err)
	}
	
	fmt.Printf("   [Node %d] 📡 Broadcast timeout for view %d\n", hc.nodeID, hc.currentView)
	return nil
}

// ProcessTimeoutMessage processes received timeout messages (protocol line 183)
func (hc *HotStuffCoordinator) ProcessTimeoutMessage(msg *messages.TimeoutMsg) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	return hc.processTimeoutMessage(msg)
}

// processTimeoutMessage handles timeout message processing
func (hc *HotStuffCoordinator) processTimeoutMessage(msg *messages.TimeoutMsg) error {
    // Validate timeout message
    if err := msg.Validate(hc.config); err != nil {
        return fmt.Errorf("invalid timeout message: %w", err)
    }

    // Verify signature
    timeoutData := fmt.Sprintf("timeout:%d:%d", msg.ViewNumber, msg.SenderID)
    if err := hc.crypto.Verify([]byte(timeoutData), msg.Signature, msg.SenderID); err != nil {
        return fmt.Errorf("invalid timeout signature: %w", err)
    }

    // Record timeout message received and validated
    hc.emitAllNodesEvent(events.EventTimeoutMessageReceived, events.EventPayload{
        "view":   msg.ViewNumber,
        "sender": msg.SenderID,
    })
    hc.emitAllNodesEvent(events.EventTimeoutMessageValidated, events.EventPayload{
        "view":   msg.ViewNumber,
        "sender": msg.SenderID,
    })
	
	// Store timeout message
	if hc.timeoutMessages[msg.ViewNumber] == nil {
		hc.timeoutMessages[msg.ViewNumber] = make(map[types.NodeID]*messages.TimeoutMsg)
	}
	hc.timeoutMessages[msg.ViewNumber][msg.SenderID] = msg
	
	// Update highest QC if this timeout has a higher QC
	if msg.HighQC != nil && (hc.highestQC == nil || msg.HighQC.View > hc.highestQC.View) {
		hc.highestQC = msg.HighQC
	}
	
	fmt.Printf("   [Node %d] 📨 Received timeout from Node %d for view %d (%d/%d timeouts)\n", 
		hc.nodeID, msg.SenderID, msg.ViewNumber, 
		len(hc.timeoutMessages[msg.ViewNumber]), hc.config.QuorumThreshold())
	
	// Check if we have enough timeouts to trigger view change (≥2f+1)
	// CRITICAL FIX: Only trigger a view change if the timeout messages are for the *current* view.
	// This prevents late messages from a previous view change attempt from causing a cascade of spurious view changes.
	if msg.ViewNumber == hc.currentView && len(hc.timeoutMessages[msg.ViewNumber]) >= hc.config.QuorumThreshold() {
		return hc.triggerViewChange(msg.ViewNumber)
	}
	
	return nil
}

// triggerViewChange advances to the next view (protocol lines 184-188)
func (hc *HotStuffCoordinator) triggerViewChange(timeoutView types.ViewNumber) error {
	// Only advance if we're still in the timeout view
	if hc.currentView != timeoutView {
		return nil // Already advanced
	}
	
	// Emit view change started event - ALL nodes should emit this
	hc.emitAllNodesEvent(events.EventViewChangeStarted, events.EventPayload{
		"old_view": hc.currentView,
		"new_view": hc.currentView + 1,
	})
	
	fmt.Printf("   [Node %d] 🔄 Triggering view change from %d to %d\n", 
		hc.nodeID, hc.currentView, hc.currentView+1)
	
	// Debug timing
	fmt.Printf("   [Node %d] ⏱️ View transition started at %v\n", hc.nodeID, time.Now().Format("15:04:05.000"))
	
    // Atomic view transition to prevent race conditions
    hc.performAtomicViewTransition()

    // Protocol line 537: "[Pacemaker] Enter synchronized view"
	// Atomic view transition already ensures synchronization
	
    // Protocol line 538: "[Pacemaker] Broadcast(NewViewMsg, view, highestQC)"
    // ALL nodes (including new leader) send NewView messages to the new leader
    currentView := hc.currentView
    newLeader := hc.getLeader(currentView)

    // Emit leader election for the new view (all nodes record this)
    hc.emitAllNodesEvent(events.EventLeaderElected, events.EventPayload{
        "view":   currentView,
        "leader": newLeader,
    })

    // Emit new_view_started for the new view (after leader_elected and timer start in performAtomicViewTransition)
    hc.emitAllNodesEvent(events.EventNewViewStarted, events.EventPayload{
        "view":   currentView,
        "leader": newLeader,
    })
	
	// Protocol line 538: "Broadcast(NewViewMsg, view, highestQC)" - ALL nodes broadcast NewView
	// Collect timeout certificates from previous view to justify view change
	var timeoutCerts []messages.TimeoutMsg
	if timeouts := hc.timeoutMessages[currentView-1]; timeouts != nil {
		for _, timeout := range timeouts {
			timeoutCerts = append(timeoutCerts, *timeout)
		}
	}
	
	// Create and broadcast NewView message (protocol line 538)
	signature := []byte("placeholder_newview_signature") // TODO: Implement proper cryptographic signing
	newViewMsg := messages.NewNewViewMsg(currentView, hc.highestQC, timeoutCerts, hc.nodeID, signature)
	
	// ALL nodes broadcast NewView as per protocol line 538
	if err := hc.network.Broadcast(hc.ctx, newViewMsg); err != nil {
		fmt.Printf("   [Node %d] ❌ Failed to broadcast NewView: %v\n", hc.nodeID, err)
		return err
	}
	
	fmt.Printf("   [Node %d] 📡 Broadcast NewView for view %d\n", hc.nodeID, currentView)
	
	if newLeader != hc.nodeID {
		// We are a validator - NewView broadcast is complete
		fmt.Printf("   [Node %d] ✅ NewView broadcast complete for view %d (validator)\n", 
			hc.nodeID, currentView)
	} else {
		// We are the new leader - prepare to collect NewView messages
		fmt.Printf("   [Node %d] 👑 Became leader for view %d, preparing for NewView collection\n", 
			hc.nodeID, currentView)
		
		// Initialize NewView collection state
		if hc.newViewMessages[currentView] == nil {
			hc.newViewMessages[currentView] = make(map[types.NodeID]*messages.NewViewMsg)
		}
		
		// Include our own NewView message in the collection since we just broadcast it
		hc.newViewMessages[currentView][hc.nodeID] = newViewMsg
		fmt.Printf("   [Node %d] 📝 Added own NewView message to collection (1/%d)\n", 
			hc.nodeID, hc.config.QuorumThreshold())
		
		// As the new leader, start the proposal process after NewView broadcast
		// Use adaptive synchronization based on network conditions instead of fixed grace period
		go func() {
			// Adaptive synchronization: Scale with view timeout to handle varying network conditions
			// Protocol specification: Allow sufficient time for distributed view transitions
			baseTimeout := hc.config.GetTimeoutForView(currentView)
			adaptiveGracePeriod := baseTimeout / 10 // 10% of view timeout for synchronization
			
			// Minimum bound for fast networks, maximum bound for slow networks
			minGrace := 100 * time.Millisecond  // Fast datacenter networks
			maxGrace := 2 * time.Second         // Slow/geographic networks
			
			if adaptiveGracePeriod < minGrace {
				adaptiveGracePeriod = minGrace
			} else if adaptiveGracePeriod > maxGrace {
				adaptiveGracePeriod = maxGrace
			}
			
			fmt.Printf("   [Node %d] ⏳ Adaptive synchronization period: %v (base timeout: %v)\n", 
				hc.nodeID, adaptiveGracePeriod, baseTimeout)
			
			select {
			case <-time.After(adaptiveGracePeriod):
				// Adaptive synchronization period elapsed, start collection
			case <-hc.ctx.Done():
				fmt.Printf("   [Node %d] ❌ Context cancelled during grace period\n", hc.nodeID)
				return
			}
			
			fmt.Printf("   [Node %d] 🚀 Starting ProposeBlock after grace period\n", hc.nodeID)
			recoveryPayload := fmt.Sprintf("recovery_block_view_%d", currentView)
			if err := hc.ProposeBlock([]byte(recoveryPayload)); err != nil {
				fmt.Printf("   [Node %d] ❌ Failed to propose recovery block for view %d: %v\n", 
					hc.nodeID, currentView, err)
			} else {
				fmt.Printf("   [Node %d] ✅ Successfully proposed recovery block for view %d\n", 
					hc.nodeID, currentView)
			}
		}()
	}
	
	return nil
}

// performAtomicViewTransition handles atomic view advancement to prevent race conditions
func (hc *HotStuffCoordinator) performAtomicViewTransition() {
	// Protocol requires atomic transition to prevent timer conflicts and message ordering races
	
	// Step 1: Stop current view timer atomically
	hc.stopViewTimer()
	
	// Step 2: Advance to next view (protocol line 522: "[Pacemaker] Advance to next view")
	nextView := hc.currentView + 1
	hc.advanceView()
	
	// Step 3: Protocol line 524: "[Pacemaker] Wait for view synchronization"
	// Start timer for new view immediately after advancing to prevent timing gaps
	hc.startViewTimer()
	
	fmt.Printf("   [Node %d] ✅ Atomic view transition completed: %d -> %d\n", 
		hc.nodeID, nextView-1, hc.currentView)
}

// broadcastNewViewMessage creates and broadcasts NewView message (protocol line 185-186)
// Removed obsolete broadcastNewViewMessage: leader announcement is not part of the diagram flow.

// ProcessNewViewMessage processes received NewView messages (protocol line 187)
func (hc *HotStuffCoordinator) ProcessNewViewMessage(msg *messages.NewViewMsg) error {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	return hc.processNewViewMessage(msg)
}

// processNewViewMessage handles NewView message processing
// According to protocol line 540-542: All nodes broadcast NewView, leader collects them
func (hc *HotStuffCoordinator) processNewViewMessage(msg *messages.NewViewMsg) error {
	currentLeader := hc.getLeader(msg.ViewNumber)
	
	// Validate NewView message first
	if err := msg.Validate(hc.config); err != nil {
		return fmt.Errorf("invalid NewView message: %w", err)
	}
	
	// Case 1: We are NOT the leader - ignore other validators' NewView messages
	// (only the leader needs to collect them)
	if currentLeader != hc.nodeID {
		// Advance to the new view if this NewView is for a higher view
		if msg.ViewNumber > hc.currentView {
			fmt.Printf("   [Node %d] 📈 Advancing to view %d based on NewView from Node %d\n", 
				hc.nodeID, msg.ViewNumber, msg.Sender())
			
			hc.stopViewTimer()
			for hc.currentView < msg.ViewNumber {
				hc.advanceView()
			}
			hc.startViewTimer()
		}
		
		// Validators don't process NewView messages further
		return nil
	}

	// Case 2: We ARE the leader for this view - collect NewView messages from all nodes
	// Protocol line 542: "Collect NewViewMsg (≥2f+1)"
	
	// Emit received event - LEADER ROLE EVENT
	hc.emitLeaderEvent(events.EventNewViewMessageReceived, events.EventPayload{
		"view":   msg.ViewNumber,
		"sender": msg.Sender(),
	})

	// Verify this is from a valid validator (including ourselves - leaders also broadcast NewView)
	isValidValidator := false
	for _, validator := range hc.validators {
		if validator == msg.Sender() {
			isValidValidator = true
			break
		}
	}
	if !isValidValidator {
		return fmt.Errorf("NewView from invalid validator: %d", msg.Sender())
	}
	
	// Emit validated event - LEADER ROLE EVENT
	hc.emitLeaderEvent(events.EventNewViewMessageValidated, events.EventPayload{
		"view":   msg.ViewNumber,
		"sender": msg.Sender(),
	})
	
	// Store NewView message
	if hc.newViewMessages[msg.ViewNumber] == nil {
		hc.newViewMessages[msg.ViewNumber] = make(map[types.NodeID]*messages.NewViewMsg)
	}
	hc.newViewMessages[msg.ViewNumber][msg.Sender()] = msg
	
	// Update highest QC if this NewView has a higher QC
	if msg.HighQC != nil && (hc.highestQC == nil || msg.HighQC.View > hc.highestQC.View) {
		hc.highestQC = msg.HighQC
		
		// Emit highest QC updated event - LEADER ROLE EVENT
		hc.emitLeaderEvent(events.EventHighestQCUpdated, events.EventPayload{
			"view":     hc.currentView,
			"qc_view":  msg.HighQC.View,
			"qc_phase": msg.HighQC.Phase,
		})
	}
	
	fmt.Printf("   [Node %d] 📨 Received NewView from Node %d for view %d\n", 
		hc.nodeID, msg.Sender(), msg.ViewNumber)
	
	// If this is for a higher view than our current, advance
	if msg.ViewNumber > hc.currentView {
		hc.stopViewTimer()
		
		// Set our view to the new view
		for hc.currentView < msg.ViewNumber {
			hc.advanceView()
		}
		
		// Start timer for new view
		hc.startViewTimer()
		
		fmt.Printf("   [Node %d] ✅ Advanced to view %d based on NewView\n", 
			hc.nodeID, hc.currentView)
	}
	
	return nil
}

// collectNewViewMessages implements the NewView collection phase (lines 195-226 in mermaid)
func (hc *HotStuffCoordinator) collectNewViewMessages() error {
	// NewView collection phase - timer event is emitted by startViewTimer() to avoid duplication

	// For view 0, still process any received NewView messages for protocol compliance
	// but don't wait for them (initial view starts immediately)
	if hc.currentView == 0 {
		// Process any NewView messages already received for view 0
		if err := hc.processReceivedNewViewMessagesForView0(); err != nil {
			fmt.Printf("   [Node %d] ⚠️ Failed to process view 0 NewView messages: %v\n", hc.nodeID, err)
		}
		fmt.Printf("   [Node %d] ⏩ Processed NewView messages for initial view 0\n", hc.nodeID)
		return nil
	}

	// Calculate required quorum for NewView messages
	quorumThreshold := hc.config.QuorumThreshold()
	fmt.Printf("   [Node %d] 📞 Collecting NewView messages (%d/%d required)\n", 
		hc.nodeID, 0, quorumThreshold)
	
	// Wait for NewView messages with timeout (protocol line 168: "2 * current_view_timeout")
	baseTimeout := hc.config.GetTimeoutForView(hc.currentView)
	timeout := 2 * baseTimeout // Protocol specifies 2x timeout for NewView collection
	deadline := time.Now().Add(timeout)
	
	fmt.Printf("   [Node %d] ⏰ NewView collection timeout: %v (2x base: %v)\n", 
		hc.nodeID, timeout, baseTimeout)
	
	// Debug: show current NewView messages at start
	initialCount := 0
	if hc.newViewMessages[hc.currentView] != nil {
		initialCount = len(hc.newViewMessages[hc.currentView])
		fmt.Printf("   [Node %d] 📊 Starting with %d NewView messages already received\n", 
			hc.nodeID, initialCount)
		for senderID := range hc.newViewMessages[hc.currentView] {
			fmt.Printf("     - From Node %d\n", senderID)
		}
	}
	
	// Wait for NewView messages with exponential backoff to reduce CPU usage
	checkInterval := 50 * time.Millisecond  // Start with 50ms intervals
	maxInterval := 500 * time.Millisecond   // Cap at 500ms intervals
	
	for time.Now().Before(deadline) {
		// Check if we have sufficient NewView messages
		if hc.newViewMessages[hc.currentView] != nil {
			receivedCount := len(hc.newViewMessages[hc.currentView])
			
			if receivedCount >= quorumThreshold {
				fmt.Printf("   [Node %d] ✅ Collected sufficient NewView messages (%d/%d)\n", 
					hc.nodeID, receivedCount, quorumThreshold)
				
				// Process collected NewView messages
				return hc.processCollectedNewViewMessages()
			}
		}
		
		// Use timeout-based waiting instead of busy polling
		select {
		case <-time.After(checkInterval):
			// Continue to next check
		case <-hc.ctx.Done():
			return fmt.Errorf("context cancelled during NewView collection")
		}
		
		// Exponential backoff: increase check interval to reduce CPU usage
		checkInterval = checkInterval * 2
		if checkInterval > maxInterval {
			checkInterval = maxInterval
		}
	}
	
	// Timeout reached without sufficient NewView messages (protocol lines 184-191)
	receivedCount := 0
	if hc.newViewMessages[hc.currentView] != nil {
		receivedCount = len(hc.newViewMessages[hc.currentView])
	}
	
	fmt.Printf("   [Node %d] ⏰ NewView collection timeout - cannot propose in view %d\n", 
		hc.nodeID, hc.currentView)
	
	// Protocol line 186: "Leader timeout - cannot propose in this view"
	// Protocol line 187-189: Sign and broadcast timeout message
	timeoutMsg := messages.NewTimeoutMsg(hc.currentView, hc.highestQC, hc.nodeID, []byte("newview_timeout"))
	
	// Store our timeout message
	if hc.timeoutMessages[hc.currentView] == nil {
		hc.timeoutMessages[hc.currentView] = make(map[types.NodeID]*messages.TimeoutMsg)
	}
	hc.timeoutMessages[hc.currentView][hc.nodeID] = timeoutMsg
	
	// Broadcast timeout message (protocol line 189)
	ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second)
	defer cancel()
	
	if err := hc.network.Broadcast(ctx, timeoutMsg); err != nil {
		fmt.Printf("   [Node %d] ❌ Failed to broadcast timeout message: %v\n", hc.nodeID, err)
	} else {
		fmt.Printf("   [Node %d] 📢 Broadcasted timeout message for view %d\n", hc.nodeID, hc.currentView)
	}
	
	// Protocol line 190-191: This will trigger view change
	return fmt.Errorf("insufficient NewView messages: received %d/%d, triggering view change", 
		receivedCount, quorumThreshold)
}

// processCollectedNewViewMessages validates and processes NewView messages
func (hc *HotStuffCoordinator) processCollectedNewViewMessages() error {
	newViewMsgs := hc.newViewMessages[hc.currentView]
	if newViewMsgs == nil {
		return fmt.Errorf("no NewView messages for current view")
	}

	validatedCount := 0
	var maxQC *types.QuorumCertificate

	// Validate each NewView message and find highest QC
	for nodeID, msg := range newViewMsgs {
		// Emit received event
		if hc.eventTracer != nil {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventNewViewMessageReceived, events.EventPayload{
				"view":   hc.currentView,
				"sender": nodeID,
			})
		}

		// Validate message signature (simplified)
		if err := msg.Validate(hc.config); err != nil {
			fmt.Printf("   [Node %d] ⚠️ Invalid NewView from Node %d: %v\n", hc.nodeID, nodeID, err)
			continue
		}

		// Emit validated event
		if hc.eventTracer != nil {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventNewViewMessageValidated, events.EventPayload{
				"view":   hc.currentView,
				"sender": nodeID,
			})
		}

		validatedCount++

		// Track highest QC
		if msg.HighQC != nil && (maxQC == nil || msg.HighQC.View > maxQC.View) {
			maxQC = msg.HighQC
		}
	}

	// Update highest QC if found
	if maxQC != nil && (hc.highestQC == nil || maxQC.View > hc.highestQC.View) {
		hc.highestQC = maxQC
		
		// Emit highest QC updated event
		if hc.eventTracer != nil {
			hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventHighestQCUpdated, events.EventPayload{
				"view":     hc.currentView,
				"qc_view":  maxQC.View,
				"qc_phase": maxQC.Phase,
			})
		}
		
		fmt.Printf("   [Node %d] 📈 Updated highest QC from NewView messages to view %d\n", 
			hc.nodeID, maxQC.View)
	}

	fmt.Printf("   [Node %d] ✅ Processed %d valid NewView messages\n", hc.nodeID, validatedCount)
	return nil
}

// processReceivedNewViewMessagesForView0 processes NewView messages for view 0
func (hc *HotStuffCoordinator) processReceivedNewViewMessagesForView0() error {
	newViewMsgs := hc.newViewMessages[0] // view 0
	if newViewMsgs == nil || len(newViewMsgs) == 0 {
		// No NewView messages yet, but still emit the required highest QC updated event for view 0
		hc.emitLeaderEvent(events.EventHighestQCUpdated, events.EventPayload{
			"view":     0,
			"qc_view":  -1, // Genesis QC has view -1
			"qc_phase": "genesis",
		})
		return nil
	}

	// Wait for NewView messages to arrive from the message processor
	// They're sent by nodes in coordinator.Start() and processed asynchronously
	maxWait := 500 * time.Millisecond
	deadline := time.Now().Add(maxWait)
	checkInterval := 50 * time.Millisecond
	
	for time.Now().Before(deadline) {
		newViewMsgs = hc.newViewMessages[0] // refresh
		if newViewMsgs != nil && len(newViewMsgs) > 0 {
			break // Messages arrived
		}
		
		// Use timeout-based waiting instead of busy polling
		select {
		case <-time.After(checkInterval):
			// Continue to next check
		case <-hc.ctx.Done():
			break
		}
	}

	if newViewMsgs != nil && len(newViewMsgs) > 0 {
		validatedCount := 0
		var maxQC *types.QuorumCertificate

		// Process received NewView messages
		for _, msg := range newViewMsgs {
			// These events are already emitted by ProcessNewViewMessage
			// Just do the processing logic here

			// Validate message (simplified for view 0)
			if err := msg.Validate(hc.config); err == nil {
				validatedCount++

				// Track highest QC
				if msg.HighQC != nil && (maxQC == nil || msg.HighQC.View > maxQC.View) {
					maxQC = msg.HighQC
				}
			}
		}

		// Update highest QC if found
		if maxQC != nil && (hc.highestQC == nil || maxQC.View > hc.highestQC.View) {
			hc.highestQC = maxQC
			
			// Emit highest QC updated event
			hc.emitLeaderEvent(events.EventHighestQCUpdated, events.EventPayload{
				"view":     0,
				"qc_view":  maxQC.View,
				"qc_phase": maxQC.Phase,
			})
		} else {
			// For view 0, emit a generic highest QC updated event even if no QC found
			// This satisfies the protocol requirement for view 0 initialization
			hc.emitLeaderEvent(events.EventHighestQCUpdated, events.EventPayload{
				"view":     0,
				"qc_view":  -1, // Genesis QC has view -1
				"qc_phase": "genesis",
			})
		}

		fmt.Printf("   [Node %d] ✅ Processed %d NewView messages for view 0\n", hc.nodeID, validatedCount)
	} else {
		// Even if no NewView messages, emit highest QC updated for view 0 initialization
		hc.emitLeaderEvent(events.EventHighestQCUpdated, events.EventPayload{
			"view":     0,
			"qc_view":  -1, // Genesis QC has view -1
			"qc_phase": "genesis",
		})
	}

	return nil
}

// sendNewViewMessageToLeader sends a NewView message to the current leader
func (hc *HotStuffCoordinator) sendNewViewMessageToLeader(view types.ViewNumber) {
	hc.mu.Lock()
	defer hc.mu.Unlock()
	
	leader := hc.getLeader(view)
	if leader == hc.nodeID {
		return // Don't send NewView to ourselves
	}

	// Create NewView message with our highest QC
	// TODO: Implement proper cryptographic signing
	signature := []byte("placeholder_signature")
	newViewMsg := messages.NewNewViewMsg(view, hc.highestQC, []messages.TimeoutMsg{}, hc.nodeID, signature)
	
	// Emit sent event
	if hc.eventTracer != nil {
		hc.eventTracer.RecordEvent(uint16(hc.nodeID), events.EventNewViewMessageSent, events.EventPayload{
			"view":   view,
			"leader": leader,
		})
	}

	// Send to leader
	if err := hc.network.Send(hc.ctx, leader, newViewMsg); err != nil {
		fmt.Printf("   [Node %d] ⚠️ Failed to send NewView to leader %d: %v\n", hc.nodeID, leader, err)
		return
	}

	fmt.Printf("   [Node %d] 📤 Sent NewView message to leader %d for view %d\n", hc.nodeID, leader, view)
}

// updateHighestQC updates the highest known QC
func (hc *HotStuffCoordinator) updateHighestQC(qc *types.QuorumCertificate) {
	if qc != nil && (hc.highestQC == nil || qc.View > hc.highestQC.View) {
		hc.highestQC = qc
		fmt.Printf("   [Node %d] 📈 Updated highest QC to view %d, phase %s\n", 
			hc.nodeID, qc.View, qc.Phase)
		
		// Note: highest_qc_updated event is emitted in NewView processing phase only
		// Not during general QC updates to avoid duplicate events
	}
}

// GetHighestQC returns the highest known QC
func (hc *HotStuffCoordinator) GetHighestQC() *types.QuorumCertificate {
	hc.mu.RLock()
	defer hc.mu.RUnlock()
	return hc.highestQC
}

// recoverFromStorage recovers the coordinator state from persistent storage
func (hc *HotStuffCoordinator) recoverFromStorage() error {
	// Recover current view
	currentView, err := hc.storage.GetCurrentView()
	if err == nil {
		hc.currentView = currentView
		fmt.Printf("   [Node %d] 📂 Recovered current view: %d\n", hc.nodeID, currentView)
	}
	
	// Recover highest QC
	highestQC, err := hc.storage.GetHighestQC()
	if err == nil && highestQC != nil {
		hc.highestQC = highestQC
		fmt.Printf("   [Node %d] 📂 Recovered highest QC: view=%d, phase=%s\n", 
			hc.nodeID, highestQC.View, highestQC.Phase)
	}
	
	// Attempt to recover QCs for recent blocks from block tree
	// This helps restore QC maps from persistent storage
	blockTree := hc.consensus.GetBlockTree()
	committedBlock := blockTree.GetCommitted()
	if committedBlock != nil {
		fmt.Printf("   [Node %d] 📂 Found committed block at height %d\n", hc.nodeID, committedBlock.Height)
		
		// Try to recover QCs for the committed block and its recent ancestors
		for phase := types.PhasePrepare; phase <= types.PhaseCommit; phase++ {
			qc, err := hc.storage.GetQC(committedBlock.Hash, phase)
			if err == nil && qc != nil {
				// Restore QC to appropriate map
				switch phase {
				case types.PhasePrepare:
					hc.prepareQCs[committedBlock.Hash] = qc
				case types.PhasePreCommit:
					hc.preCommitQCs[committedBlock.Hash] = qc
				case types.PhaseCommit:
					hc.commitQCs[committedBlock.Hash] = qc
				}
				fmt.Printf("   [Node %d] 📂 Recovered %s QC for block %x\n", 
					hc.nodeID, phase, committedBlock.Hash[:8])
			}
		}
	}
	
	fmt.Printf("   [Node %d] ✅ Storage recovery completed\n", hc.nodeID)
	return nil
}
